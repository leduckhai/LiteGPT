{
    "run": {
        "task": "image_text_pretrain",
        "lr_sched": "linear_warmup_cosine_lr",
        "init_lr": 1e-05,
        "min_lr": 1e-06,
        "warmup_lr": 1e-06,
        "weight_decay": 0.05,
        "max_epoch": 3,
        "num_workers": 2,
        "warmup_steps": 1000,
        "iters_per_epoch": 3750,
        "seed": 42,
        "output_dir": "checkpoint",
        "amp": true,
        "resume_ckpt_path": null,
        "evaluate": false,
        "train_splits": [
            "train"
        ],
        "device": "cuda",
        "world_size": 1,
        "dist_url": "env://",
        "distributed": false,
        "wandb_log": true,
        "wandb_token": "ebc667f949a72cc90bd5fd1546629e1e8d677487",
        "job_name": "pointllm_finetune_scannet"
    },
    "model": {
        "arch": "pointvlm",
        "vision_model": "point_transformer",
        "image_size": 224,
        "drop_path_rate": 0,
        "use_grad_checkpoint": true,
        "precision": "fp16",
        "freeze_vision": true,
        "prompt": "",
        "language_model": "meta-llama/Llama-2-7b-chat-hf",
        "lora_r": 64,
        "lora_alpha": 16,
        "model_type": "pretrain",
        "max_txt_len": 1024,
        "end_sym": "</s>",
        "ckpt": "",
        "chat_template": true,
        "low_resource": true,
        "bits": 8
    },
    "preprocess": {
        "vis_processor": {
            "train": {
                "name": "blip2_image_train",
                "image_size": 448
            },
            "eval": {
                "name": "blip2_image_eval",
                "image_size": 448
            }
        },
        "text_processor": {
            "train": {
                "name": "blip_caption"
            },
            "eval": {
                "name": "blip_caption"
            }
        }
    },
    "datasets": {
        "pointvlm_train": {
            "data_type": "images",
            "build_info": {
                "image_path": "data/Scannet/scans",
                "ann_path": "data/qa/ScanQA_v1.0_train.json",
                "num_points": 200000
            },
            "batch_size": 4,
            "num_points": 200000,
            "text_processor": {
                "train": {
                    "name": "blip_caption"
                }
            },
            "sample_ratio": 10
        }
    }
}
{
    "run": {
        "task": "image_text_pretrain",
        "lr_sched": "linear_warmup_cosine_lr",
        "init_lr": 1e-05,
        "min_lr": 1e-06,
        "warmup_lr": 1e-06,
        "weight_decay": 0.05,
        "max_epoch": 3,
        "num_workers": 2,
        "warmup_steps": 1000,
        "iters_per_epoch": 3750,
        "seed": 42,
        "output_dir": "checkpoint",
        "amp": true,
        "resume_ckpt_path": null,
        "evaluate": false,
        "train_splits": [
            "train"
        ],
        "device": "cuda",
        "world_size": 1,
        "dist_url": "env://",
        "distributed": false,
        "wandb_log": true,
        "wandb_token": "ebc667f949a72cc90bd5fd1546629e1e8d677487",
        "job_name": "pointllm_finetune_scannet"
    },
    "model": {
        "arch": "pointvlm",
        "vision_model": "point_transformer",
        "image_size": 224,
        "drop_path_rate": 0,
        "use_grad_checkpoint": true,
        "precision": "fp16",
        "freeze_vision": true,
        "prompt": "",
        "language_model": "meta-llama/Llama-2-7b-chat-hf",
        "lora_r": 64,
        "lora_alpha": 16,
        "model_type": "pretrain",
        "max_txt_len": 1024,
        "end_sym": "</s>",
        "ckpt": "",
        "chat_template": true,
        "low_resource": true,
        "bits": 8
    },
    "preprocess": {
        "vis_processor": {
            "train": {
                "name": "blip2_image_train",
                "image_size": 448
            },
            "eval": {
                "name": "blip2_image_eval",
                "image_size": 448
            }
        },
        "text_processor": {
            "train": {
                "name": "blip_caption"
            },
            "eval": {
                "name": "blip_caption"
            }
        }
    },
    "datasets": {
        "pointvlm_train": {
            "data_type": "images",
            "build_info": {
                "image_path": "data/Scannet/scans",
                "ann_path": "data/qa/ScanQA_v1.0_train.json",
                "num_points": 200000
            },
            "batch_size": 4,
            "num_points": 200000,
            "text_processor": {
                "train": {
                    "name": "blip_caption"
                }
            },
            "sample_ratio": 10
        }
    }
}
{
    "run": {
        "task": "image_text_pretrain",
        "lr_sched": "linear_warmup_cosine_lr",
        "init_lr": 1e-05,
        "min_lr": 1e-06,
        "warmup_lr": 1e-06,
        "weight_decay": 0.05,
        "max_epoch": 3,
        "num_workers": 2,
        "warmup_steps": 1000,
        "iters_per_epoch": 3750,
        "seed": 42,
        "output_dir": "checkpoint",
        "amp": true,
        "resume_ckpt_path": null,
        "evaluate": false,
        "train_splits": [
            "train"
        ],
        "device": "cuda",
        "world_size": 1,
        "dist_url": "env://",
        "distributed": false,
        "wandb_log": true,
        "wandb_token": "ebc667f949a72cc90bd5fd1546629e1e8d677487",
        "job_name": "pointllm_finetune_scannet"
    },
    "model": {
        "arch": "pointvlm",
        "vision_model": "point_transformer",
        "image_size": 224,
        "drop_path_rate": 0,
        "use_grad_checkpoint": true,
        "precision": "fp16",
        "freeze_vision": true,
        "prompt": "",
        "language_model": "meta-llama/Llama-2-7b-chat-hf",
        "lora_r": 64,
        "lora_alpha": 16,
        "model_type": "pretrain",
        "max_txt_len": 1024,
        "end_sym": "</s>",
        "ckpt": "",
        "chat_template": true,
        "low_resource": true,
        "bits": 8
    },
    "preprocess": {
        "vis_processor": {
            "train": {
                "name": "blip2_image_train",
                "image_size": 448
            },
            "eval": {
                "name": "blip2_image_eval",
                "image_size": 448
            }
        },
        "text_processor": {
            "train": {
                "name": "blip_caption"
            },
            "eval": {
                "name": "blip_caption"
            }
        }
    },
    "datasets": {
        "pointvlm_train": {
            "data_type": "images",
            "build_info": {
                "image_path": "data/Scannet/scans",
                "ann_path": "data/qa/ScanQA_v1.0_train.json",
                "num_points": 200000
            },
            "batch_size": 4,
            "num_points": 200000,
            "text_processor": {
                "train": {
                    "name": "blip_caption"
                }
            },
            "sample_ratio": 10
        }
    }
}
{
    "run": {
        "task": "image_text_pretrain",
        "lr_sched": "linear_warmup_cosine_lr",
        "init_lr": 1e-05,
        "min_lr": 1e-06,
        "warmup_lr": 1e-06,
        "weight_decay": 0.05,
        "max_epoch": 3,
        "num_workers": 2,
        "warmup_steps": 1000,
        "iters_per_epoch": 3750,
        "seed": 42,
        "output_dir": "checkpoint",
        "amp": true,
        "resume_ckpt_path": null,
        "evaluate": false,
        "train_splits": [
            "train"
        ],
        "device": "cuda",
        "world_size": 1,
        "dist_url": "env://",
        "distributed": false,
        "wandb_log": true,
        "wandb_token": "ebc667f949a72cc90bd5fd1546629e1e8d677487",
        "job_name": "pointllm_finetune_scannet"
    },
    "model": {
        "arch": "pointvlm",
        "vision_model": "point_transformer",
        "image_size": 224,
        "drop_path_rate": 0,
        "use_grad_checkpoint": true,
        "precision": "fp16",
        "freeze_vision": true,
        "prompt": "",
        "language_model": "meta-llama/Llama-2-7b-chat-hf",
        "lora_r": 64,
        "lora_alpha": 16,
        "model_type": "pretrain",
        "max_txt_len": 1024,
        "end_sym": "</s>",
        "ckpt": "",
        "chat_template": true,
        "low_resource": true,
        "bits": 8
    },
    "preprocess": {
        "vis_processor": {
            "train": {
                "name": "blip2_image_train",
                "image_size": 448
            },
            "eval": {
                "name": "blip2_image_eval",
                "image_size": 448
            }
        },
        "text_processor": {
            "train": {
                "name": "blip_caption"
            },
            "eval": {
                "name": "blip_caption"
            }
        }
    },
    "datasets": {
        "pointvlm_train": {
            "data_type": "images",
            "build_info": {
                "image_path": "data/Scannet/scans",
                "ann_path": "data/qa/ScanQA_v1.0_train.json",
                "num_points": 200000
            },
            "batch_size": 4,
            "num_points": 200000,
            "text_processor": {
                "train": {
                    "name": "blip_caption"
                }
            },
            "sample_ratio": 10
        }
    }
}
